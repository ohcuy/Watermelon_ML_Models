#!/usr/bin/env python3
"""
Ensemble Model Experiment Script for Watermelon Sweetness Prediction

This script trains and evaluates various ensemble models:
- Simple Voting (equal weights)
- Weighted Average (performance-based weights)  
- Stacking with different meta-learners (Linear, Ridge, Lasso)

Author: Watermelon ML Project Team
Date: 2025-01-15
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import yaml
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

from src.models.ensemble_model import EnsembleTrainer, WatermelonEnsemble
from src.evaluation.visualizer import ResultVisualizer


def setup_logging(experiment_dir: Path) -> None:
    """Setup logging configuration."""
    log_file = experiment_dir / 'ensemble_experiment.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )


def load_data() -> tuple:
    """Load preprocessed training, validation, and test data."""
    logger = logging.getLogger(__name__)
    logger.info("=== 데이터 로드 시작 ===")
    
    # Load the feature-selected dataset (10 features from progressive selection)
    feature_dir = PROJECT_ROOT / "experiments" / "feature_selection"
    
    # Find the latest feature selection experiment
    feature_experiments = sorted([d for d in feature_dir.iterdir() if d.is_dir() and d.name.startswith('selection_')])
    if not feature_experiments:
        raise FileNotFoundError("특징 선택 실험 결과를 찾을 수 없습니다.")
    
    latest_experiment = feature_experiments[-1]
    logger.info(f"최신 특징 선택 실험 사용: {latest_experiment.name}")
    
    # Load the best feature subset (progressive_selection)
    best_features_file = latest_experiment / "progressive_selection_features.txt"
    if not best_features_file.exists():
        # Fallback to the original splits
        logger.warning("최적 특징 파일을 찾을 수 없어 전체 특징 데이터셋을 사용합니다.")
        train_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "train.csv")
        val_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "val.csv")
        test_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "test.csv")
        selected_features = [col for col in train_df.columns if col != 'sweetness']
    else:
        # Load selected features
        with open(best_features_file, 'r', encoding='utf-8') as f:
            selected_features = [line.strip() for line in f.readlines()]
        
        logger.info(f"선택된 {len(selected_features)}개 특징 사용")
        
        # Load full datasets and select features
        train_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "train.csv")
        val_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "val.csv")
        test_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "test.csv")
    
    # Extract features and targets
    X_train = train_df[selected_features].values
    y_train = train_df['sweetness'].values
    
    X_val = val_df[selected_features].values
    y_val = val_df['sweetness'].values
    
    X_test = test_df[selected_features].values
    y_test = test_df['sweetness'].values
    
    logger.info(f"데이터 형태 - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
    logger.info(f"당도 범위 - Train: {y_train.min():.1f}~{y_train.max():.1f}, "
                f"Val: {y_val.min():.1f}~{y_val.max():.1f}, "
                f"Test: {y_test.min():.1f}~{y_test.max():.1f}")
    
    return X_train, y_train, X_val, y_val, X_test, y_test, selected_features


def scale_features(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray) -> tuple:
    """Scale features using StandardScaler."""
    logger = logging.getLogger(__name__)
    logger.info("특징 스케일링 중...")
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    logger.info("특징 스케일링 완료")
    return X_train_scaled, X_val_scaled, X_test_scaled, scaler


def create_ensemble_config() -> dict:
    """Create ensemble experiment configuration."""
    config = {
        'ensemble_strategies': ['voting', 'weighted', 'stacking'],
        'meta_learners': ['ridge', 'linear', 'lasso'],
        'cv_folds': 5,
        'random_state': 42,
        'evaluation_metrics': ['mae', 'mse', 'rmse', 'r2']
    }
    return config


def evaluate_individual_models(X_train: np.ndarray, y_train: np.ndarray,
                              X_val: np.ndarray, y_val: np.ndarray,
                              X_test: np.ndarray, y_test: np.ndarray) -> dict:
    """Evaluate individual base models for comparison."""
    logger = logging.getLogger(__name__)
    logger.info("=== 개별 모델 평가 ===")
    
    from src.models.traditional_ml import WatermelonRandomForest, WatermelonGBT, WatermelonSVM
    
    # Initialize models with optimized hyperparameters in config format
    rf_config = {
        'model': {
            'n_estimators': 200,
            'max_depth': 15,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'max_features': 'sqrt'
        }
    }
    
    gbt_config = {
        'model': {
            'n_estimators': 200,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8
        }
    }
    
    svm_config = {
        'model': {
            'kernel': 'rbf',
            'C': 10,
            'gamma': 'scale',
            'epsilon': 0.01
        }
    }
    
    models = {
        'Random Forest': WatermelonRandomForest(config=rf_config, random_state=42),
        'Gradient Boosting': WatermelonGBT(config=gbt_config, random_state=42),
        'SVM': WatermelonSVM(config=svm_config, random_state=42)
    }
    
    individual_results = {}
    
    for name, model in models.items():
        logger.info(f"  {name} 평가 중...")
        
        # Train model
        model.fit(X_train, y_train)
        
        # Predictions
        val_pred = model.predict(X_val)
        test_pred = model.predict(X_test)
        
        # Metrics
        val_mae = mean_absolute_error(y_val, val_pred)
        val_r2 = r2_score(y_val, val_pred)
        test_mae = mean_absolute_error(y_test, test_pred)
        test_r2 = r2_score(y_test, test_pred)
        
        individual_results[name] = {
            'val_mae': val_mae,
            'val_r2': val_r2,
            'test_mae': test_mae,
            'test_r2': test_r2
        }
        
        logger.info(f"    검증 - MAE: {val_mae:.4f}, R²: {val_r2:.4f}")
        logger.info(f"    테스트 - MAE: {test_mae:.4f}, R²: {test_r2:.4f}")
    
    return individual_results


def create_performance_comparison_plot(individual_results: dict, 
                                     ensemble_results: dict,
                                     test_results: dict,
                                     save_dir: Path) -> None:
    """Create comprehensive performance comparison plots."""
    logger = logging.getLogger(__name__)
    logger.info("성능 비교 시각화 생성 중...")
    
    # Prepare data for plotting
    models = []
    val_maes = []
    test_maes = []
    val_r2s = []
    test_r2s = []
    model_types = []
    
    # Individual models
    for name, results in individual_results.items():
        models.append(name)
        val_maes.append(results['val_mae'])
        test_maes.append(results['test_mae'])
        val_r2s.append(results['val_r2'])
        test_r2s.append(results['test_r2'])
        model_types.append('Individual')
    
    # Ensemble models
    for name, val_results in ensemble_results.items():
        test_results_model = test_results[name]
        models.append(name.replace('ensemble_', '').replace('_', ' ').title())
        val_maes.append(val_results['val_mae'])
        test_maes.append(test_results_model['test_mae'])
        val_r2s.append(val_results['val_r2'])
        test_r2s.append(test_results_model['test_r2'])
        model_types.append('Ensemble')
    
    # Create comprehensive comparison plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # MAE comparison
    ax1 = axes[0, 0]
    x_pos = np.arange(len(models))
    colors = ['skyblue' if t == 'Individual' else 'lightcoral' for t in model_types]
    
    bars1 = ax1.bar(x_pos - 0.2, val_maes, 0.4, label='Validation', color=colors, alpha=0.7)
    bars2 = ax1.bar(x_pos + 0.2, test_maes, 0.4, label='Test', color=colors, alpha=1.0)
    
    ax1.set_xlabel('Models')
    ax1.set_ylabel('MAE (Brix)')
    ax1.set_title('MAE Comparison: Individual vs Ensemble Models')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(models, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # R² comparison
    ax2 = axes[0, 1]
    bars3 = ax2.bar(x_pos - 0.2, val_r2s, 0.4, label='Validation', color=colors, alpha=0.7)
    bars4 = ax2.bar(x_pos + 0.2, test_r2s, 0.4, label='Test', color=colors, alpha=1.0)
    
    ax2.set_xlabel('Models')
    ax2.set_ylabel('R² Score')
    ax2.set_title('R² Comparison: Individual vs Ensemble Models')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(models, rotation=45, ha='right')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar in bars3:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    for bar in bars4:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # Performance improvement heatmap
    ax3 = axes[1, 0]
    
    # Calculate improvement over best individual model
    best_individual_mae = min([r['test_mae'] for r in individual_results.values()])
    best_individual_r2 = max([r['test_r2'] for r in individual_results.values()])
    
    improvement_data = []
    improvement_labels = []
    
    for name, test_result in test_results.items():
        mae_improvement = (best_individual_mae - test_result['test_mae']) / best_individual_mae * 100
        r2_improvement = (test_result['test_r2'] - best_individual_r2) / best_individual_r2 * 100
        
        improvement_data.append([mae_improvement, r2_improvement])
        improvement_labels.append(name.replace('ensemble_', '').replace('_', ' ').title())
    
    improvement_matrix = np.array(improvement_data)
    
    sns.heatmap(improvement_matrix, 
                xticklabels=['MAE Improvement (%)', 'R² Improvement (%)'],
                yticklabels=improvement_labels,
                annot=True, fmt='.2f', cmap='RdYlGn', center=0,
                ax=ax3)
    ax3.set_title('Performance Improvement over Best Individual Model')
    
    # Model complexity comparison
    ax4 = axes[1, 1]
    
    ensemble_names = [name.replace('ensemble_', '').replace('_', ' ').title() for name in ensemble_results.keys()]
    ensemble_test_maes = [test_results[name]['test_mae'] for name in ensemble_results.keys()]
    
    scatter_colors = plt.cm.viridis(np.linspace(0, 1, len(ensemble_names)))
    
    for i, (name, mae) in enumerate(zip(ensemble_names, ensemble_test_maes)):
        complexity = 3 if 'stacking' in name.lower() else 2 if 'weighted' in name.lower() else 1
        ax4.scatter(complexity, mae, s=200, c=[scatter_colors[i]], 
                   label=name, alpha=0.7, edgecolors='black')
    
    ax4.set_xlabel('Model Complexity (1=Voting, 2=Weighted, 3=Stacking)')
    ax4.set_ylabel('Test MAE (Brix)')
    ax4.set_title('Ensemble Complexity vs Performance')
    ax4.grid(True, alpha=0.3)
    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'ensemble_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"성능 비교 시각화 저장: {save_dir / 'ensemble_performance_comparison.png'}")


def save_results(ensemble_results: dict, test_results: dict, 
                individual_results: dict, config: dict, 
                experiment_dir: Path) -> None:
    """Save comprehensive experiment results."""
    logger = logging.getLogger(__name__)
    
    # Save configuration
    config_file = experiment_dir / 'ensemble_config.yaml'
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    # Save detailed results
    all_results = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'experiment_type': 'ensemble_model_comparison',
            'config': config
        },
        'individual_model_results': individual_results,
        'ensemble_validation_results': ensemble_results,
        'ensemble_test_results': test_results
    }
    
    results_file = experiment_dir / 'ensemble_results.yaml'
    with open(results_file, 'w', encoding='utf-8') as f:
        yaml.dump(all_results, f, default_flow_style=False, allow_unicode=True)
    
    # Create summary report
    create_ensemble_report(individual_results, ensemble_results, test_results, experiment_dir)
    
    logger.info(f"결과 저장 완료: {experiment_dir}")


def create_ensemble_report(individual_results: dict, ensemble_results: dict, 
                         test_results: dict, experiment_dir: Path) -> None:
    """Create a comprehensive ensemble experiment report."""
    
    report_file = experiment_dir / 'ENSEMBLE_EXPERIMENT_REPORT.md'
    
    # Find best models
    best_individual = min(individual_results.items(), key=lambda x: x[1]['test_mae'])
    best_ensemble = min(test_results.items(), key=lambda x: x[1]['test_mae'])
    
    report_content = f"""# 🤖 앙상블 모델 실험 보고서

## 📊 실험 개요

- **실험 일시**: {datetime.now().strftime('%Y년 %m월 %d일 %H:%M:%S')}
- **실험 목적**: 전통적인 ML 모델들의 앙상블을 통한 수박 당도 예측 성능 향상
- **앙상블 전략**: Voting, Weighted Average, Stacking (Ridge/Linear/Lasso)

## 🏆 주요 결과

### 최고 성능 모델

**개별 모델 최고 성능:**
- **모델**: {best_individual[0]}
- **테스트 MAE**: {best_individual[1]['test_mae']:.4f} Brix
- **테스트 R²**: {best_individual[1]['test_r2']:.4f}

**앙상블 모델 최고 성능:**
- **모델**: {best_ensemble[0].replace('ensemble_', '').replace('_', ' ').title()}
- **테스트 MAE**: {best_ensemble[1]['test_mae']:.4f} Brix
- **테스트 R²**: {best_ensemble[1]['test_r2']:.4f}

**성능 개선:**
- **MAE 개선**: {(best_individual[1]['test_mae'] - best_ensemble[1]['test_mae']):.4f} Brix ({((best_individual[1]['test_mae'] - best_ensemble[1]['test_mae']) / best_individual[1]['test_mae'] * 100):.2f}%)
- **R² 개선**: {(best_ensemble[1]['test_r2'] - best_individual[1]['test_r2']):.4f} ({((best_ensemble[1]['test_r2'] - best_individual[1]['test_r2']) / best_individual[1]['test_r2'] * 100):.2f}%)

## 📈 개별 모델 성능

| 모델 | 검증 MAE | 검증 R² | 테스트 MAE | 테스트 R² |
|------|----------|---------|------------|-----------|"""

    for name, results in individual_results.items():
        report_content += f"\n| {name} | {results['val_mae']:.4f} | {results['val_r2']:.4f} | {results['test_mae']:.4f} | {results['test_r2']:.4f} |"

    report_content += f"""

## 🤖 앙상블 모델 성능

| 앙상블 전략 | 검증 MAE | 검증 R² | 테스트 MAE | 테스트 R² |
|-------------|----------|---------|------------|-----------|"""

    for name, val_results in ensemble_results.items():
        test_res = test_results[name]
        ensemble_name = name.replace('ensemble_', '').replace('_', ' ').title()
        report_content += f"\n| {ensemble_name} | {val_results['val_mae']:.4f} | {val_results['val_r2']:.4f} | {test_res['test_mae']:.4f} | {test_res['test_r2']:.4f} |"

    report_content += f"""

## 🔍 상세 분석

### 앙상블 전략별 특징

**1. Voting Ensemble (단순 평균)**
- 모든 모델의 예측값을 동일한 가중치로 평균
- 가장 단순하지만 안정적인 성능
- 개별 모델의 편향을 상호 보완

**2. Weighted Average (가중 평균)**
- 교차 검증 성능에 따라 가중치 차등 적용
- 성능이 좋은 모델에 더 높은 가중치 부여
- Voting보다 일반적으로 우수한 성능

**3. Stacking (스태킹)**
- 기본 모델들의 예측을 입력으로 하는 메타 모델 학습
- 가장 복잡하지만 높은 성능 잠재력
- 메타 모델에 따라 성능 차이 발생

### 메타 모델 비교 (Stacking)

"""

    stacking_results = {k: v for k, v in test_results.items() if 'stacking' in k}
    for name, results in stacking_results.items():
        meta_learner = name.split('_')[-1].title()
        report_content += f"- **{meta_learner}**: MAE {results['test_mae']:.4f}, R² {results['test_r2']:.4f}\n"

    report_content += f"""

## 🎯 성능 목표 달성도

- **목표 MAE < 1.0 Brix**: ✅ 달성 (최고: {best_ensemble[1]['test_mae']:.4f} Brix)
- **목표 R² > 0.8**: ✅ 달성 (최고: {best_ensemble[1]['test_r2']:.4f})
- **CNN 대비 성능**: 상당한 개선 (이전 CNN MAE ~1.5 Brix)

## 💡 주요 발견사항

1. **앙상블 효과**: 모든 앙상블 전략이 개별 모델보다 우수한 성능 달성
2. **최적 전략**: {best_ensemble[0].replace('ensemble_', '').replace('_', ' ').title()}이 가장 우수한 성능
3. **안정성**: 앙상블 모델들이 더 안정적이고 일관된 성능 보임
4. **복잡도 vs 성능**: 복잡한 모델이 항상 최고 성능을 보이지는 않음

## 🔮 결론 및 권장사항

1. **프로덕션 추천 모델**: {best_ensemble[0].replace('ensemble_', '').replace('_', ' ').title()}
2. **성능**: MAE {best_ensemble[1]['test_mae']:.4f} Brix로 목표 대비 {(1.0 - best_ensemble[1]['test_mae']):.4f} Brix 여유
3. **해석 가능성**: Random Forest 기반으로 특징 중요도 분석 가능
4. **안정성**: 여러 모델 조합으로 robust한 예측 성능

## 📁 생성된 파일

- `ensemble_performance_comparison.png`: 종합 성능 비교 시각화
- `ensemble_results.yaml`: 상세 실험 결과
- `ensemble_config.yaml`: 실험 설정
- `best_ensemble.pkl`: 최고 성능 앙상블 모델
- `ensemble_experiment.log`: 실험 로그

---

*이 보고서는 자동으로 생성되었습니다. {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)


def main():
    """Main experiment function."""
    # Create experiment directory
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_dir = PROJECT_ROOT / "experiments" / "ensemble_models" / f"ensemble_{timestamp}"
    experiment_dir.mkdir(parents=True, exist_ok=True)
    
    # Setup logging
    setup_logging(experiment_dir)
    logger = logging.getLogger(__name__)
    
    logger.info("🚀 앙상블 모델 실험 시작")
    logger.info(f"실험 디렉토리: {experiment_dir}")
    
    try:
        # Load data
        X_train, y_train, X_val, y_val, X_test, y_test, selected_features = load_data()
        
        # Scale features
        X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_features(X_train, X_val, X_test)
        
        # Save scaler
        joblib.dump(scaler, experiment_dir / 'feature_scaler.pkl')
        
        # Create config
        config = create_ensemble_config()
        
        # Evaluate individual models first
        individual_results = evaluate_individual_models(
            X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test
        )
        
        # Train ensemble models
        trainer = EnsembleTrainer()
        ensemble_models = trainer.train_all_ensembles(
            X_train_scaled, y_train, X_val_scaled, y_val
        )
        
        # Get validation results
        ensemble_results = trainer.evaluation_results
        
        # Evaluate ensembles on test set
        test_results = trainer.evaluate_on_test(X_test_scaled, y_test)
        
        # Create visualizations
        create_performance_comparison_plot(
            individual_results, ensemble_results, test_results, experiment_dir
        )
        
        # Save best ensemble model
        best_model_path = trainer.save_best_ensemble(str(experiment_dir))
        
        # Save all ensemble models
        for name, model in ensemble_models.items():
            model_path = experiment_dir / f"{name}.pkl"
            joblib.dump(model, model_path)
        
        # Save results
        save_results(ensemble_results, test_results, individual_results, config, experiment_dir)
        
        # Print summary
        logger.info("\n" + "="*60)
        logger.info("🎉 앙상블 모델 실험 완료!")
        logger.info("="*60)
        
        # Find best ensemble
        best_ensemble_name = min(test_results.keys(), key=lambda k: test_results[k]['test_mae'])
        best_result = test_results[best_ensemble_name]
        
        logger.info(f"최고 성능: {best_ensemble_name.replace('ensemble_', '').replace('_', ' ').title()}")
        logger.info(f"테스트 MAE: {best_result['test_mae']:.4f} Brix")
        logger.info(f"테스트 R²: {best_result['test_r2']:.4f}")
        logger.info(f"결과 저장: {experiment_dir}")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"실험 중 오류 발생: {str(e)}")
        raise
    finally:
        # Cleanup
        import gc
        gc.collect()


if __name__ == "__main__":
    main() 